{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4d85da",
   "metadata": {},
   "source": [
    "# SWOT data acquisition\n",
    "\n",
    "#### Jonas Felipe Santos de Souza (jonas.ssouza@ufpe.br)\n",
    "\n",
    "#### Federal University of Pernambuco\n",
    "\n",
    "#### June 10, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9a891",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9d85f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> verifica if the libraries below are installed.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c1eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, DrawControl, GeoJSON, Popup, Rectangle\n",
    "from shapely.geometry import Polygon, LineString\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import HTML\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import earthaccess\n",
    "import warnings\n",
    "import zipfile\n",
    "import time\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbcef66-3180-490d-afbc-d168d9a6bfff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipyleaflet in c:\\users\\crist\\anaconda3\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: branca>=0.5.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipyleaflet) (0.8.1)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipyleaflet) (8.1.7)\n",
      "Requirement already satisfied: jupyter-leaflet<0.21,>=0.20 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipyleaflet) (0.20.0)\n",
      "Requirement already satisfied: traittypes<3,>=0.2.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipyleaflet) (0.2.1)\n",
      "Requirement already satisfied: xyzservices>=2021.8.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipyleaflet) (2022.9.0)\n",
      "Requirement already satisfied: jinja2>=3 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from branca>=0.5.0->ipyleaflet) (3.1.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipyleaflet) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipyleaflet) (8.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipyleaflet) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipyleaflet) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipywidgets<9,>=7.6.0->ipyleaflet) (3.0.15)\n",
      "Requirement already satisfied: decorator in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\crist\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from jinja2>=3->branca>=0.5.0->ipyleaflet) (2.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\crist\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\crist\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\crist\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\crist\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\crist\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets<9,>=7.6.0->ipyleaflet) (1.16.0)\n",
      "Requirement already satisfied: shapely in c:\\users\\crist\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from shapely) (1.26.4)\n",
      "Requirement already satisfied: geopandas in c:\\users\\crist\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from geopandas) (0.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\crist\\anaconda3\\lib\\site-packages (from geopandas) (24.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from geopandas) (2.2.2)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from geopandas) (3.7.1)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from geopandas) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->geopandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->geopandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pyogrio>=0.7.2->geopandas) (2026.1.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->geopandas) (1.16.0)\n",
      "Requirement already satisfied: earthaccess in c:\\users\\crist\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2022.11 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (2024.6.1)\n",
      "Requirement already satisfied: importlib-resources>=6.3.2 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (6.5.2)\n",
      "Requirement already satisfied: multimethod>=1.8 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (2.0)\n",
      "Requirement already satisfied: pqdm>=0.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (0.2.0)\n",
      "Requirement already satisfied: python-cmr>=0.10.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (0.13.0)\n",
      "Requirement already satisfied: requests>=2.26 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (2.32.3)\n",
      "Requirement already satisfied: s3fs>=2022.11 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (2024.6.1)\n",
      "Requirement already satisfied: tinynetrc>=1.3.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from earthaccess) (4.11.0)\n",
      "Requirement already satisfied: bounded-pool-executor in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pqdm>=0.1->earthaccess) (0.0.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\crist\\anaconda3\\lib\\site-packages (from pqdm>=0.1->earthaccess) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from python-cmr>=0.10.0->earthaccess) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from requests>=2.26->earthaccess) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from requests>=2.26->earthaccess) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from requests>=2.26->earthaccess) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from requests>=2.26->earthaccess) (2026.1.4)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from s3fs>=2022.11->earthaccess) (2.12.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from s3fs>=2022.11->earthaccess) (3.10.5)\n",
      "Requirement already satisfied: botocore<1.34.70,>=1.34.41 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2022.11->earthaccess) (1.34.69)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2022.11->earthaccess) (1.14.1)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2022.11->earthaccess) (0.7.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2022.11->earthaccess) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2022.11->earthaccess) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2022.11->earthaccess) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2022.11->earthaccess) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2022.11->earthaccess) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2022.11->earthaccess) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.8.2->python-cmr>=0.10.0->earthaccess) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\crist\\anaconda3\\lib\\site-packages (from tqdm->pqdm>=0.1->earthaccess) (0.4.6)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\crist\\anaconda3\\lib\\site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs>=2022.11->earthaccess) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipyleaflet\n",
    "!pip install shapely\n",
    "!pip install geopandas\n",
    "!pip install earthaccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569881d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc294c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIRECTORY\n",
    "inpath = 'C:/Users/crist/Desktop/Doctorado/paper/SWOT/swot_rivers/'\n",
    "\n",
    "# Path to save the obtained products\n",
    "swotpath = f'{inpath}products/' # *.zip\n",
    "\n",
    "# SWOT ID of river sections (*.csv file)\n",
    "swot_id = f'{inpath}nodos_valdivia.csv'\n",
    "\n",
    "# Path to save SWOT data after extraction\n",
    "swot_data = f'{inpath}valdivia/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedeca33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## River_SP SWOT database acquisition\n",
    "\n",
    "La base de datos debe obtenerse de the *hydroweb.next* platform (https://hydroweb.next.theia-land.fr/).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>SKIP THIS STEP IF YOU ALREADY HAVE THE DATABASE YOU ARE INTERESTED IN.</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209210e-d7ef-4793-aff2-d1590319e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py-hydroweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb70fa2-bc62-4187-a4c5-741002c71091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U py-hydroweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740206e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "help_message = \"\"\"\n",
    "Download products from your hydroweb.next projects (https://hydroweb.next.theia-land.fr) using the py-hydroweb lib (https://pypi.org/project/py-hydroweb/)\n",
    "This script is an example tuned for your last hydroweb.next project but feel free to adapt it for future requests.\n",
    "Follow these steps:\n",
    "1. If not already done, install py-hydroweb latest version using `pip install -U py-hydroweb` (WARNING: python >= 3.8 is required)\n",
    "2a. Generate an API-Key from hydroweb.next portal in your user settings\n",
    "2b. Carefully store your API-Key (2 options):\n",
    "- either in an environment variable `export HYDROWEB_API_KEY=\"<your_key_here>\"`\n",
    "- or in below script by replacing <your_key_here>\n",
    "3. You can change download directory by adding an `output_folder` parameter when calling `submit_and_download_zip` (see below). By default, current path is used.\n",
    "4. You are all set, run this script `python download_script.py`\n",
    "\n",
    "For more documentation about how to use the py-hydroweb lib, please refer to https://pypi.org/project/py-hydroweb/.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52af1ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from importlib.metadata import version\n",
    "help_message = \"\"\"\n",
    "Error: py_hydroweb no está instalado.\n",
    "Por favor instálalo con:\n",
    "    pip install py-hydroweb\n",
    "\"\"\"\n",
    "try:\n",
    "    import py_hydroweb\n",
    "except ImportError:\n",
    "    print(help_message)\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2895c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check py-hydroweb version\n",
    "import logging\n",
    "from importlib.metadata import version, PackageNotFoundError  # Python 3.8+\n",
    "\n",
    "latest_version = \"1.0.2\"\n",
    "package_name = \"py-hydroweb\"\n",
    "\n",
    "try:\n",
    "    current_version = version(package_name)\n",
    "    if current_version < latest_version:\n",
    "        logging.getLogger().warning(f\"\"\"\\033[33m\n",
    "/!\\\\ Consider upgrading {package_name} to {latest_version} using `pip install -U {package_name}`\n",
    "(Current version: {current_version})\n",
    "\\033[0m\"\"\")\n",
    "    else:\n",
    "        print(f\"{package_name} is up to date (v{current_version})\")\n",
    "except PackageNotFoundError:\n",
    "    logging.getLogger().warning(f\"\"\"\\033[31m\n",
    "/!\\\\ The package {package_name} is not installed. Install it using `pip install {package_name}`\n",
    "\\033[0m\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0de8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set log config\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fa558",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> Verifica in your hydroweb.next account if the <b>API key</b> has been generated and is active.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98daead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key\n",
    "api_hydroweb = \"eMwM9406V62a88130Ydx3Y7uWi4ZAtIVsMCPxg7I7AqcHArVMd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc285496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client\n",
    "#  - either using the API-Key environment variable (HYDROWEB_API_KEY)\n",
    "#client: py_hydroweb.Client = py_hydroweb.Client(\"https://hydroweb.next.theia-land.fr/api\")\n",
    "#  - or explicitly giving API-Key (comment line above and uncomment line below)\n",
    "client: py_hydroweb.Client = py_hydroweb.Client(\"https://hydroweb.next.theia-land.fr/api\", \n",
    "                                                api_key=api_hydroweb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new download basket (input the name you want here)\n",
    "basket: py_hydroweb.DownloadBasket = py_hydroweb.DownloadBasket(\"biobio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add collections in our basket\n",
    "# Inserte las coordenadas de la región de interés en el campo bbox\n",
    "# \"SWOT_PRIOR_RIVER_DATABASE\"\n",
    "# \"SWOT_PRIOR_LAKE_DATABASE\"\n",
    "basket.add_collection(\"SWOT_PRIOR_RIVER_DATABASE\", \n",
    "        #bbox=[-41.40, -9.60, -34.74, -7.10])\n",
    "        bbox=[-74.091797, -38.894373, -70.378418, -36.300877])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do download (input the archive name you want here, and optionally an output folder)\n",
    "now = datetime.today().strftime(\"%Y%m%dT%H%M%S\")\n",
    "downloaded_zip_path: str = client.submit_and_download_zip(\n",
    "    basket,\n",
    "    zip_filename=f\"{inpath}my_hydroweb_data_{now}.zip\",\n",
    "    #, output_folder = \"<change_me>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136983f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "635fff74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SWOT product search data configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3eb8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapefile with reaches river\n",
    "# This file should be obtained from the SWOT database at hydroweb.next\n",
    "shp = f'{inpath}SWOT_PRIOR_RIVER_DATABASE/sa_sword_nodes_hb66_v17b.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c5a44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para busca dos produtos SWOT\n",
    "# 'SWOT_L2_HR_LakeSP_Prior_2.0', 'SWOT_L2_HR_RiverSP_Reach_2.0', 'SWOT_L2_HR_Raster_100m_2.0'\n",
    "swot_product = 'SWOT_L2_HR_RiverSP_node_2.0'\n",
    "short_product = 'SWOT_RiverSP' # 'SWOT_LakeSP', 'SWOT_Raster', 'SWOT_RiverSP'\n",
    "date_start = '2023-01-01'\n",
    "date_end = '2026-01-01'\n",
    "granule_product = '*'\n",
    "\n",
    "# Plot graphs\n",
    "ifplot = True # True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24969e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mapa para seleccionar el área de ​​interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe4bced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a map centered on the Amazon region\n",
    "m = Map(center=(-7.900, -35.250), zoom=8, layout={'height': '600px', 'width': '1000px'}, scroll_wheel_zoom=True)\n",
    "\n",
    "# Global variables to store the drawn polygon, the GeoJSON layer, and the rectangle layer\n",
    "polygon = None\n",
    "popup = None\n",
    "geojson_layer = None\n",
    "rectangle_layer = None\n",
    "\n",
    "# Step 2: Function to handle draw events\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global polygon, geojson_layer, rectangle_layer\n",
    "    if action == 'created':\n",
    "        # Clear existing GeoJSON layer if it exists\n",
    "        if geojson_layer:\n",
    "            m.remove_layer(geojson_layer)\n",
    "\n",
    "        # Clear the existing rectangle layer if it exists\n",
    "        if rectangle_layer:\n",
    "            m.remove_layer(rectangle_layer)\n",
    "\n",
    "        # Clear the existing polygon layer if it exists\n",
    "        if polygon:\n",
    "            polygon_layer = GeoJSON(data={'type': 'Feature', 'geometry': polygon.__geo_interface__})\n",
    "            m.remove_layer(polygon_layer)\n",
    "\n",
    "        # Capture the polygon geometry\n",
    "        geometry = geo_json['geometry']\n",
    "        polygon = Polygon(geometry['coordinates'][0])\n",
    "        print(\"Polygon drawn:\", polygon)\n",
    "\n",
    "        # Draw the rectangle on the map\n",
    "        minx, miny, maxx, maxy = polygon.bounds\n",
    "        rectangle_layer = Rectangle(bounds=((miny, minx), (maxy, maxx)), color='blue', fill_opacity=0.1)\n",
    "        m.add_layer(rectangle_layer)\n",
    "\n",
    "        # Draw the new polygon layer\n",
    "        polygon_layer = GeoJSON(data={'type': 'Feature', 'geometry': polygon.__geo_interface__})\n",
    "        m.add_layer(polygon_layer)\n",
    "\n",
    "        load_shapefile()  # Load the shapefile based on the drawn polygon\n",
    "    elif action == 'deleted':\n",
    "        polygon = None\n",
    "        if geojson_layer:\n",
    "            m.remove_layer(geojson_layer)\n",
    "            geojson_layer = None\n",
    "        if rectangle_layer:\n",
    "            m.remove_layer(rectangle_layer)\n",
    "            rectangle_layer = None  # Reset rectangle_layer to None\n",
    "\n",
    "# Step 3: Load and filter the shapefile based on the drawn polygon\n",
    "def load_shapefile():\n",
    "    global polygon, geojson_layer\n",
    "    if polygon:\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp)\n",
    "            print(f\"Shapefile carregado com sucesso: {len(gdf)} registros encontrados.\")\n",
    "            \n",
    "            gdf = gdf.to_crs(epsg=4326)\n",
    "            filtered_gdf = gdf[gdf.intersects(polygon)]\n",
    "            print(f\"{len(filtered_gdf)} registros encontrados dentro da área desenhada.\")\n",
    "            \n",
    "            geojson_data = filtered_gdf.__geo_interface__\n",
    "            \n",
    "            geojson_layer = GeoJSON(data=geojson_data, style={'color': 'green', 'opacity': 0.8, 'weight': 2})\n",
    "\n",
    "            # Add hover event to display 'lake_if' in a pop-up\n",
    "            def on_hover(event, feature, **kwargs):\n",
    "                global popup\n",
    "                lake_id = feature['properties'].get('reach_id', 'N/A')\n",
    "\n",
    "                geom = feature['geometry']\n",
    "                if geom['type'] == 'Polygon' or geom['type'] == 'MultiPolygon':\n",
    "                    coords = Polygon(geom['coordinates'][0]).centroid.coords[0]\n",
    "                elif geom['type'] == 'LineString':\n",
    "                    coords = LineString(geom['coordinates']).centroid.coords[0]\n",
    "                else:\n",
    "                    coords = geom['coordinates']\n",
    "\n",
    "                # Create content for the popup with a selectable 'lake_id'\n",
    "                html_content = HTML(f'<div style=\"font-size: 14px;\"><b>Reach ID:</b> <span style=\"user-select: text;\">{lake_id}</span></div>')\n",
    "\n",
    "                # Remove the previous popup if it exists\n",
    "                if popup:\n",
    "                    m.remove_layer(popup)\n",
    "\n",
    "                # Create a new popup with the lake_id and add it to the map\n",
    "                popup = Popup(\n",
    "                    location=(coords[1], coords[0]),  # Coordinates (lat, lon)\n",
    "                    child=html_content,\n",
    "                    close_button=False,\n",
    "                    auto_close=False,\n",
    "                    close_on_escape_key=False\n",
    "                )\n",
    "                m.add_layer(popup)\n",
    "\n",
    "            geojson_layer.on_hover(on_hover)\n",
    "\n",
    "            m.add_layer(geojson_layer)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar o shapefile: {e}\")\n",
    "\n",
    "# Step 4: Add drawing tools to the map, allowing only rectangles\n",
    "draw_control = DrawControl(\n",
    "    rectangle={'shapeOptions': {'color': '#0000FF'}},  \n",
    "    polyline={}, polygon={}, circle={}, marker={}, circlemarker={}\n",
    ")\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# Step 5: Display the map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e065d0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7f5af11c2b46e990bdac8db2683ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-7.9, -35.25], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title', 'zoom_o…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7029f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Inicio de sesión y búsqueda de productos SWOT in the *EarthData* database\n",
    "\n",
    "Necesitas una cuenta de EarthData (https://urs.earthdata.nasa.gov/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d74c49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> Por favor make sure your EarthData account login and password are correct. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd13695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "polygon.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998e7bde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se dibujaron polígonos.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'items' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo se dibujaron polígonos.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Display the granules found\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(items))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(items)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'items' is not defined"
     ]
    }
   ],
   "source": [
    "# Comprueba si se ha dibujado el polígono\n",
    "if 'polygon' in globals():   \n",
    "    # Earthdata login\n",
    "    earthaccess.login()\n",
    "    \n",
    "    # Obtener datos dentro de los límites del polígono\n",
    "    results = earthaccess.search_data(short_name = swot_product,\n",
    "                                      temporal = (date_start, date_end),\n",
    "                                      #granule_name=granule_product,\n",
    "                                      bounding_box=(polygon.bounds))\n",
    "    \n",
    "    # Mostrar los granules encontrados\n",
    "    items = [item['meta']['native-id'] for item in results]\n",
    "    #print(f\"Granules encontrados: {items}\")\n",
    "else:\n",
    "    print(\"No se dibujaron polígonos.\")\n",
    "\n",
    "# Display the granules found\n",
    "print(len(items))\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b1a36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Descarga de datos SWOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba0a33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Robust download with retries for transient network errors (IncompleteRead / ChunkedEncodingError)\n",
    "def _granule_zip_name(granule):\n",
    "    try:\n",
    "        return f\"{granule['meta']['native-id']}.zip\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _is_valid_zip(path):\n",
    "    if (not path.exists()) or path.stat().st_size == 0:\n",
    "        return False\n",
    "    try:\n",
    "        with zipfile.ZipFile(path, 'r') as zf:\n",
    "            return zf.testzip() is None\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_file_with_retries(granule, download_path, max_retries=5, base_wait=5):\n",
    "    download_dir = Path(download_path)\n",
    "    download_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    expected_name = _granule_zip_name(granule)\n",
    "    expected_file = download_dir / expected_name if expected_name else None\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            earthaccess.download(granule, str(download_dir))\n",
    "\n",
    "            # Prefer expected file name when available\n",
    "            candidate = expected_file\n",
    "            if candidate is None or not candidate.exists():\n",
    "                matches = sorted(download_dir.glob('SWOT*.zip'), key=os.path.getmtime)\n",
    "                candidate = matches[-1] if matches else None\n",
    "\n",
    "            if candidate and _is_valid_zip(candidate):\n",
    "                print(f\"✅ Download OK: {candidate.name}\")\n",
    "                return True\n",
    "\n",
    "            if candidate and candidate.exists():\n",
    "                candidate.unlink(missing_ok=True)\n",
    "            raise IOError(\"Downloaded file is missing, incomplete, or corrupted.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt}/{max_retries} failed for {expected_name or 'granule'}: {e}\")\n",
    "            # Remove partial target file before retrying\n",
    "            if expected_file and expected_file.exists():\n",
    "                expected_file.unlink(missing_ok=True)\n",
    "\n",
    "            if attempt < max_retries:\n",
    "                wait_s = base_wait * attempt\n",
    "                time.sleep(wait_s)\n",
    "            else:\n",
    "                print(f\"❌ Failed after {max_retries} attempts: {expected_name or 'granule'}\")\n",
    "                return False\n",
    "\n",
    "\n",
    "# Download files with retries\n",
    "file_urls = results[:500]\n",
    "failed = []\n",
    "for granule in file_urls:\n",
    "    ok = download_file_with_retries(granule, swotpath)\n",
    "    if not ok:\n",
    "        failed.append(_granule_zip_name(granule) or str(granule))\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\nFailed downloads: {len(failed)}\")\n",
    "    for f in failed[:20]:\n",
    "        print(' -', f)\n",
    "\n",
    "# Check the most recent file in the download directory\n",
    "files = glob.glob(swotpath + 'SWOT*.zip')\n",
    "try:\n",
    "    file = max(files, key=os.path.getmtime)\n",
    "    print(f\"\\nThe most recent file is: {file}\")\n",
    "except ValueError:\n",
    "    print(\"\\nNo files were downloaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18dc96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Captura de datos de interés from downloaded SWOT products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a200dc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> Es útil tener un archivo *.csv with data on the sections of the rivers of interest (name, reaches SWOT ID etc.).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed2639-330b-4c31-8f06-aaa1e7f62f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *.csv file with SWOT IDs of rivers NODES\n",
    "nodes_df = pd.read_csv(swot_id, sep=';', decimal=',')\n",
    "if 'node ID' not in nodes_df.columns:\n",
    "    raise ValueError(\"El CSV debe tener una columna 'node ID'.\")\n",
    "nodes_df['node ID'] = nodes_df['node ID'].astype(str)\n",
    "nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8d935-fc1c-4351-8b7d-f8d068c8b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tempfile import mkdtemp  # <--- ESTA LÍNEA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11ebd0-63ca-4e48-b9f0-0abae0e9b9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990c9b9-2923-497e-ad63-d8dfcbfa846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "from tempfile import TemporaryDirectory\n",
    "from collections import defaultdict\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parámetros ---\n",
    "zip_dir     = swotpath          # carpeta con los .zip de SWOT\n",
    "output_dir  = swot_data         # carpeta de salida\n",
    "short_product = \"Node\"          # etiqueta para el archivo de salida\n",
    "ifplot      = True              # True = graficar, False = solo exportar\n",
    "valid_node_q = [0, 1]           # calidades válidas\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Lista de node_id a procesar (tal cual vienen en tu CSV/DataFrame) ---\n",
    "nodes_order = list(nodes_df['node ID'])\n",
    "nodes_set   = set(nodes_order)\n",
    "\n",
    "# --- Acumulador por node_id ---\n",
    "# NodeID -> [(Date, wse, wse_u, width, slope, slope_u, width_u, area_total, area_tot_u, area_detct, area_det_u, node_dist), ...]\n",
    "accum = defaultdict(list)\n",
    "\n",
    "# --- Recorre cada ZIP una sola vez ---\n",
    "for filename in os.listdir(zip_dir):\n",
    "    if not (filename.lower().endswith(\".zip\") and \"node\" in filename.lower()):\n",
    "        continue\n",
    "    zip_path = os.path.join(zip_dir, filename)\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z, TemporaryDirectory(prefix=\"swot_nodes_\") as tmp_dir:\n",
    "            z.extractall(tmp_dir)\n",
    "\n",
    "            # Busca .shp (incluye subcarpetas)\n",
    "            shp_files = []\n",
    "            for root, _, files in os.walk(tmp_dir):\n",
    "                shp_files += [os.path.join(root, f) for f in files if f.lower().endswith(\".shp\")]\n",
    "            if not shp_files:\n",
    "                continue\n",
    "\n",
    "            for shp_path in shp_files:\n",
    "                try:\n",
    "                    gdf = gpd.read_file(shp_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] No se pudo leer {shp_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Mapeo case-insensitive\n",
    "                cols = {c.lower(): c for c in gdf.columns}\n",
    "                time_col      = cols.get('time_str') or cols.get('time')\n",
    "                node_id_col   = cols.get('node_id')\n",
    "                node_q_col    = cols.get('node_q')\n",
    "                wse_col       = cols.get('wse')\n",
    "                wse_u_col     = cols.get('wse_u')\n",
    "                width_col     = cols.get('width')\n",
    "                slope_col     = cols.get('slope')\n",
    "                slope_u_col   = cols.get('slope_u')\n",
    "                width_u_col   = cols.get('width_u') or cols.get('wifth_u')\n",
    "                area_total_col= cols.get('area_total')\n",
    "                area_tot_u_col= cols.get('area_tot_u')\n",
    "                area_detct_col= cols.get('area_detct')\n",
    "                area_det_u_col= cols.get('area_det_u')\n",
    "                node_dist_col = cols.get('node_dist')\n",
    "\n",
    "                if not (node_id_col and node_q_col and wse_col and time_col):\n",
    "                    continue\n",
    "\n",
    "                # Filtra temprano por tus node_id y calidad válida\n",
    "                mask = gdf[node_id_col].isin(nodes_set) & gdf[node_q_col].isin(valid_node_q)\n",
    "\n",
    "                # Columnas a conservar\n",
    "                keep_cols = [node_id_col, time_col, wse_col]\n",
    "                optional_cols = [\n",
    "                    wse_u_col, width_col, slope_col, slope_u_col, width_u_col,\n",
    "                    area_total_col, area_tot_u_col, area_detct_col, area_det_u_col, node_dist_col\n",
    "                ]\n",
    "                keep_cols += [c for c in optional_cols if c]\n",
    "\n",
    "                sub = gdf.loc[mask, keep_cols].copy()\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "\n",
    "                # Limpia y acumula\n",
    "                sub[\"Date\"] = pd.to_datetime(sub[time_col], errors=\"coerce\")\n",
    "                sub[\"wse\"] = pd.to_numeric(sub[wse_col], errors=\"coerce\")\n",
    "\n",
    "                def add_numeric_col(df, out_name, in_name):\n",
    "                    if in_name:\n",
    "                        df[out_name] = pd.to_numeric(df[in_name], errors=\"coerce\")\n",
    "                    else:\n",
    "                        df[out_name] = pd.NA\n",
    "\n",
    "                add_numeric_col(sub, \"wse_u\", wse_u_col)\n",
    "                add_numeric_col(sub, \"width\", width_col)\n",
    "                add_numeric_col(sub, \"slope\", slope_col)\n",
    "                add_numeric_col(sub, \"slope_u\", slope_u_col)\n",
    "                add_numeric_col(sub, \"width_u\", width_u_col)\n",
    "                add_numeric_col(sub, \"area_total\", area_total_col)\n",
    "                add_numeric_col(sub, \"area_tot_u\", area_tot_u_col)\n",
    "                add_numeric_col(sub, \"area_detct\", area_detct_col)\n",
    "                add_numeric_col(sub, \"area_det_u\", area_det_u_col)\n",
    "                add_numeric_col(sub, \"node_dist\", node_dist_col)\n",
    "\n",
    "                sub = sub.dropna(subset=[\"Date\", \"wse\"])\n",
    "\n",
    "                for row in sub[[\n",
    "                    node_id_col, \"Date\", \"wse\", \"wse_u\", \"width\", \"slope\", \"slope_u\", \"width_u\",\n",
    "                    \"area_total\", \"area_tot_u\", \"area_detct\", \"area_det_u\", \"node_dist\"\n",
    "                ]].itertuples(index=False, name=None):\n",
    "                    nid, dt, wse, wse_u, width, slope, slope_u, width_u, area_total, area_tot_u, area_detct, area_det_u, node_dist = row\n",
    "                    accum[nid].append((dt, wse, wse_u, width, slope, slope_u, width_u, area_total, area_tot_u, area_detct, area_det_u, node_dist))\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"[WARN] ZIP corrupto o inválido: {zip_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Error procesando {zip_path}: {e}\")\n",
    "\n",
    "# --- Exporta y grafica SOLO los node_id de nodes_df (en su orden) ---\n",
    "for node_id in nodes_order:\n",
    "    pairs = accum.get(node_id, [])\n",
    "    if not pairs:\n",
    "        print(f\"No data found for node_id {node_id}.\")\n",
    "        continue\n",
    "\n",
    "    pairs.sort(key=lambda x: x[0])  # ordenar por fecha\n",
    "    df = pd.DataFrame(\n",
    "        pairs,\n",
    "        columns=[\"Date\", \"wse\", \"wse_u\", \"width\", \"slope\", \"slope_u\", \"width_u\", \"area_total\", \"area_tot_u\", \"area_detct\", \"area_det_u\", \"node_dist\"]\n",
    "    )\n",
    "\n",
    "    output_csv = os.path.join(output_dir, f\"{node_id}_{short_product}.csv\")\n",
    "    df.to_csv(output_csv, sep=',', decimal='.', encoding='utf-8', index=False)\n",
    "    print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "    if ifplot:\n",
    "        # Gráfico se mantiene SOLO con WSE\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df[\"Date\"], df[\"wse\"], marker='o', linestyle='-')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('WSE (m) EGM08')\n",
    "        plt.title(f'WSE over Time for Node ID: {node_id}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e378d-3db2-48d1-a8d4-8a5ec1a4aafb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())  # Muestra la carpeta actual de trabajo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2cca-771e-44dd-b1d0-ed35dc9ac523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
