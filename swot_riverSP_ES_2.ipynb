{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4d85da",
   "metadata": {},
   "source": [
    "# SWOT data acquisition\n",
    "\n",
    "#### Jonas Felipe Santos de Souza (jonas.ssouza@ufpe.br)\n",
    "\n",
    "#### Federal University of Pernambuco\n",
    "\n",
    "#### June 10, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9a891",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9d85f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> verifica if the libraries below are installed.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, DrawControl, GeoJSON, Popup, Rectangle\n",
    "from shapely.geometry import Polygon\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import HTML\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import earthaccess\n",
    "import warnings\n",
    "import zipfile\n",
    "import time\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcef66-3180-490d-afbc-d168d9a6bfff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ipyleaflet\n",
    "!pip install shapely\n",
    "!pip install geopandas\n",
    "!pip install earthaccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569881d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc294c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIRECTORY\n",
    "inpath = 'C:/Users/crist/Desktop/Doctorado/paper/SWOT/swot_rivers/'\n",
    "\n",
    "# Path to save the obtained products\n",
    "swotpath = f'{inpath}products/' # *.zip\n",
    "\n",
    "# SWOT ID of river sections (*.csv file)\n",
    "swot_id = f'{inpath}nodoscuencabiobio.csv'\n",
    "\n",
    "# Path to save SWOT data after extraction\n",
    "swot_data = f'{inpath}River_SP_v17/nodes/todascalidades'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedeca33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "## River_SP SWOT database acquisition\n",
    "\n",
    "La base de datos debe obtenerse de the *hydroweb.next* platform (https://hydroweb.next.theia-land.fr/).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>SKIP THIS STEP IF YOU ALREADY HAVE THE DATABASE YOU ARE INTERESTED IN.</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209210e-d7ef-4793-aff2-d1590319e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py-hydroweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb70fa2-bc62-4187-a4c5-741002c71091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U py-hydroweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740206e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "help_message = \"\"\"\n",
    "Download products from your hydroweb.next projects (https://hydroweb.next.theia-land.fr) using the py-hydroweb lib (https://pypi.org/project/py-hydroweb/)\n",
    "This script is an example tuned for your last hydroweb.next project but feel free to adapt it for future requests.\n",
    "Follow these steps:\n",
    "1. If not already done, install py-hydroweb latest version using `pip install -U py-hydroweb` (WARNING: python >= 3.8 is required)\n",
    "2a. Generate an API-Key from hydroweb.next portal in your user settings\n",
    "2b. Carefully store your API-Key (2 options):\n",
    "- either in an environment variable `export HYDROWEB_API_KEY=\"<your_key_here>\"`\n",
    "- or in below script by replacing <your_key_here>\n",
    "3. You can change download directory by adding an `output_folder` parameter when calling `submit_and_download_zip` (see below). By default, current path is used.\n",
    "4. You are all set, run this script `python download_script.py`\n",
    "\n",
    "For more documentation about how to use the py-hydroweb lib, please refer to https://pypi.org/project/py-hydroweb/.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52af1ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from importlib.metadata import version\n",
    "help_message = \"\"\"\n",
    "Error: py_hydroweb no está instalado.\n",
    "Por favor instálalo con:\n",
    "    pip install py-hydroweb\n",
    "\"\"\"\n",
    "try:\n",
    "    import py_hydroweb\n",
    "except ImportError:\n",
    "    print(help_message)\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b2895c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check py-hydroweb version\n",
    "import logging\n",
    "from importlib.metadata import version, PackageNotFoundError  # Python 3.8+\n",
    "\n",
    "latest_version = \"1.0.2\"\n",
    "package_name = \"py-hydroweb\"\n",
    "\n",
    "try:\n",
    "    current_version = version(package_name)\n",
    "    if current_version < latest_version:\n",
    "        logging.getLogger().warning(f\"\"\"\\033[33m\n",
    "/!\\\\ Consider upgrading {package_name} to {latest_version} using `pip install -U {package_name}`\n",
    "(Current version: {current_version})\n",
    "\\033[0m\"\"\")\n",
    "    else:\n",
    "        print(f\"{package_name} is up to date (v{current_version})\")\n",
    "except PackageNotFoundError:\n",
    "    logging.getLogger().warning(f\"\"\"\\033[31m\n",
    "/!\\\\ The package {package_name} is not installed. Install it using `pip install {package_name}`\n",
    "\\033[0m\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0de8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set log config\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3fa558",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> Verifica in your hydroweb.next account if the <b>API key</b> has been generated and is active.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98daead0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# API key\n",
    "api_hydroweb = \"eMwM9406V62a88130Ydx3Y7uWi4ZAtIVsMCPxg7I7AqcHArVMd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc285496",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a client\n",
    "#  - either using the API-Key environment variable (HYDROWEB_API_KEY)\n",
    "#client: py_hydroweb.Client = py_hydroweb.Client(\"https://hydroweb.next.theia-land.fr/api\")\n",
    "#  - or explicitly giving API-Key (comment line above and uncomment line below)\n",
    "client: py_hydroweb.Client = py_hydroweb.Client(\"https://hydroweb.next.theia-land.fr/api\", \n",
    "                                                api_key=api_hydroweb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879f9cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initiate a new download basket (input the name you want here)\n",
    "basket: py_hydroweb.DownloadBasket = py_hydroweb.DownloadBasket(\"biobio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fabb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add collections in our basket\n",
    "# Inserte las coordenadas de la región de interés en el campo bbox\n",
    "# \"SWOT_PRIOR_RIVER_DATABASE\"\n",
    "# \"SWOT_PRIOR_LAKE_DATABASE\"\n",
    "basket.add_collection(\"SWOT_PRIOR_RIVER_DATABASE\", \n",
    "        #bbox=[-41.40, -9.60, -34.74, -7.10])\n",
    "        bbox=[-74.091797, -38.894373, -70.378418, -36.300877])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do download (input the archive name you want here, and optionally an output folder)\n",
    "now = datetime.today().strftime(\"%Y%m%dT%H%M%S\")\n",
    "downloaded_zip_path: str = client.submit_and_download_zip(\n",
    "    basket,\n",
    "    zip_filename=f\"{inpath}my_hydroweb_data_{now}.zip\",\n",
    "    #, output_folder = \"<change_me>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a136983f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "635fff74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SWOT product search data configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapefile with reaches river\n",
    "# This file should be obtained from the SWOT database at hydroweb.next\n",
    "shp = f'{inpath}SWOT_PRIOR_RIVER_DATABASE/sa_sword_nodes_hb66_v17.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a44ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dados para busca dos produtos SWOT\n",
    "# 'SWOT_L2_HR_LakeSP_Prior_2.0', 'SWOT_L2_HR_RiverSP_Reach_2.0', 'SWOT_L2_HR_Raster_100m_2.0'\n",
    "swot_product = 'SWOT_L2_HR_RiverSP_node_2.0'\n",
    "short_product = 'SWOT_RiverSP' # 'SWOT_LakeSP', 'SWOT_Raster', 'SWOT_RiverSP'\n",
    "date_start = '2022-12-16'\n",
    "date_end = '2025-07-16'\n",
    "granule_product = '*'\n",
    "\n",
    "# Plot graphs\n",
    "ifplot = True # True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24969e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Mapa para seleccionar el área de ​​interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a map centered on the Amazon region\n",
    "m = Map(center=(-7.900, -35.250), zoom=8, layout={'height': '600px', 'width': '1000px'}, scroll_wheel_zoom=True)\n",
    "\n",
    "# Global variables to store the drawn polygon, the GeoJSON layer, and the rectangle layer\n",
    "polygon = None\n",
    "popup = None\n",
    "geojson_layer = None\n",
    "rectangle_layer = None\n",
    "\n",
    "# Step 2: Function to handle draw events\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global polygon, geojson_layer, rectangle_layer\n",
    "    if action == 'created':\n",
    "        # Clear existing GeoJSON layer if it exists\n",
    "        if geojson_layer:\n",
    "            m.remove_layer(geojson_layer)\n",
    "\n",
    "        # Clear the existing rectangle layer if it exists\n",
    "        if rectangle_layer:\n",
    "            m.remove_layer(rectangle_layer)\n",
    "\n",
    "        # Clear the existing polygon layer if it exists\n",
    "        if polygon:\n",
    "            polygon_layer = GeoJSON(data={'type': 'Feature', 'geometry': polygon.__geo_interface__})\n",
    "            m.remove_layer(polygon_layer)\n",
    "\n",
    "        # Capture the polygon geometry\n",
    "        geometry = geo_json['geometry']\n",
    "        polygon = Polygon(geometry['coordinates'][0])\n",
    "        print(\"Polygon drawn:\", polygon)\n",
    "\n",
    "        # Draw the rectangle on the map\n",
    "        bounds = geometry['coordinates'][0]\n",
    "        rectangle_layer = Rectangle(bounds=bounds, color='blue', fill_opacity=0.1)\n",
    "        m.add_layer(rectangle_layer)\n",
    "\n",
    "        # Draw the new polygon layer\n",
    "        polygon_layer = GeoJSON(data={'type': 'Feature', 'geometry': polygon.__geo_interface__})\n",
    "        m.add_layer(polygon_layer)\n",
    "\n",
    "        load_shapefile()  # Load the shapefile based on the drawn polygon\n",
    "    elif action == 'deleted':\n",
    "        polygon = None\n",
    "        if geojson_layer:\n",
    "            m.remove_layer(geojson_layer)\n",
    "            geojson_layer = None\n",
    "        if rectangle_layer:\n",
    "            m.remove_layer(rectangle_layer)\n",
    "            rectangle_layer = None  # Reset rectangle_layer to None\n",
    "\n",
    "# Step 3: Load and filter the shapefile based on the drawn polygon\n",
    "def load_shapefile():\n",
    "    global polygon, geojson_layer\n",
    "    if polygon:\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp)\n",
    "            print(f\"Shapefile carregado com sucesso: {len(gdf)} registros encontrados.\")\n",
    "            \n",
    "            gdf = gdf.to_crs(epsg=4326)\n",
    "            filtered_gdf = gdf[gdf.intersects(polygon)]\n",
    "            print(f\"{len(filtered_gdf)} registros encontrados dentro da área desenhada.\")\n",
    "            \n",
    "            geojson_data = filtered_gdf.__geo_interface__\n",
    "            \n",
    "            geojson_layer = GeoJSON(data=geojson_data, style={'color': 'green', 'opacity': 0.8, 'weight': 2})\n",
    "\n",
    "            # Add hover event to display 'lake_if' in a pop-up\n",
    "            def on_hover(event, feature, **kwargs):\n",
    "                global popup\n",
    "                lake_id = feature['properties'].get('reach_id', 'N/A')\n",
    "\n",
    "                geom = feature['geometry']\n",
    "                if geom['type'] == 'Polygon' or geom['type'] == 'MultiPolygon':\n",
    "                    coords = Polygon(geom['coordinates'][0]).centroid.coords[0]\n",
    "                elif geom['type'] == 'LineString':\n",
    "                    coords = Polygon(geom['coordinates']).centroid.coords[0]\n",
    "                else:\n",
    "                    coords = geom['coordinates']\n",
    "\n",
    "                # Create content for the popup with a selectable 'lake_id'\n",
    "                html_content = HTML(f'<div style=\"font-size: 14px;\"><b>Reach ID:</b> <span style=\"user-select: text;\">{lake_id}</span></div>')\n",
    "\n",
    "                # Remove the previous popup if it exists\n",
    "                if popup:\n",
    "                    m.remove_layer(popup)\n",
    "\n",
    "                # Create a new popup with the lake_id and add it to the map\n",
    "                popup = Popup(\n",
    "                    location=(coords[1], coords[0]),  # Coordinates (lat, lon)\n",
    "                    child=html_content,\n",
    "                    close_button=False,\n",
    "                    auto_close=False,\n",
    "                    close_on_escape_key=False\n",
    "                )\n",
    "                m.add_layer(popup)\n",
    "\n",
    "            geojson_layer.on_hover(on_hover)\n",
    "\n",
    "            m.add_layer(geojson_layer)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar o shapefile: {e}\")\n",
    "\n",
    "# Step 4: Add drawing tools to the map, allowing only rectangles\n",
    "draw_control = DrawControl(\n",
    "    rectangle={'shapeOptions': {'color': '#0000FF'}},  \n",
    "    polyline={}, polygon={}, circle={}, marker={}, circlemarker={}\n",
    ")\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# Step 5: Display the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7029f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Inicio de sesión y búsqueda de productos SWOT in the *EarthData* database\n",
    "\n",
    "Necesitas una cuenta de EarthData (https://urs.earthdata.nasa.gov/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d74c49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> Por favor make sure your EarthData account login and password are correct. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd13695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "polygon.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e7bde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Comprueba si se ha dibujado el polígono\n",
    "if 'polygon' in globals():   \n",
    "    # Earthdata login\n",
    "    earthaccess.login()\n",
    "    \n",
    "    # Obtener datos dentro de los límites del polígono\n",
    "    results = earthaccess.search_data(short_name = swot_product,\n",
    "                                      temporal = (date_start, date_end),\n",
    "                                      #granule_name=granule_product,\n",
    "                                      bounding_box=(polygon.bounds))\n",
    "    \n",
    "    # Mostrar los granules encontrados\n",
    "    items = [item['meta']['native-id'] for item in results]\n",
    "    #print(f\"Granules encontrados: {items}\")\n",
    "else:\n",
    "    print(\"No se dibujaron polígonos.\")\n",
    "\n",
    "# Display the granules found\n",
    "print(len(items))\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b1a36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Descarga de datos SWOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba0a33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to download a file and retry if necessary\n",
    "def download_file_with_retries(file_url, download_path, max_retries=3):\n",
    "    attempts = 0\n",
    "    success = False\n",
    "\n",
    "    while attempts < max_retries and not success:\n",
    "        try:\n",
    "            earthaccess.download(file_url, download_path)\n",
    "            file = max(Path(download_path).glob('SWOT*.zip'), key=os.path.getmtime)\n",
    "            if file.exists() and file.stat().st_size > 0:\n",
    "                success = True\n",
    "                print(f\"Successfully downloaded: {file}\")\n",
    "            else:\n",
    "                raise Exception(\"File downloaded but appears to be incomplete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1} failed: {e}\")\n",
    "            attempts += 1\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "            # Clean up the incomplete file\n",
    "    if not success:\n",
    "        print(f\"Failed to download file after {max_retries} attempts.\")\n",
    "\n",
    "# Set the download directory and file URL (list of URLs in this example)\n",
    "file_urls = results[:500]  # Replace with your actual list of file URLs\n",
    "\n",
    "# Download files with retries\n",
    "for file_url in file_urls:\n",
    "    download_file_with_retries(file_url, swotpath)\n",
    "\n",
    "# Check the most recent file in the download directory\n",
    "files = glob.glob(swotpath + 'SWOT*.zip')\n",
    "try:\n",
    "    file = max(files, key=os.path.getmtime)\n",
    "    print(f\"\\nThe most recent file is: {file}\")\n",
    "    #assert file.exists()\n",
    "except ValueError:\n",
    "    print(\"\\nNo files were downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18dc96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Captura de datos de interés from downloaded SWOT products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a200dc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANTE:</b> Es útil tener un archivo *.csv with data on the sections of the rivers of interest (name, reaches SWOT ID etc.).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed2639-330b-4c31-8f06-aaa1e7f62f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *.csv file with SWOT IDs of rivers NODES\n",
    "nodes_df = pd.read_csv(swot_id, sep=';', decimal=',')\n",
    "if 'node ID' not in nodes_df.columns:\n",
    "    raise ValueError(\"El CSV debe tener una columna 'node ID'.\")\n",
    "nodes_df['node ID'] = nodes_df['node ID'].astype(str)\n",
    "nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8d935-fc1c-4351-8b7d-f8d068c8b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tempfile import mkdtemp  # <--- ESTA LÍNEA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11ebd0-63ca-4e48-b9f0-0abae0e9b9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990c9b9-2923-497e-ad63-d8dfcbfa846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "from tempfile import TemporaryDirectory\n",
    "from collections import defaultdict\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parámetros ---\n",
    "zip_dir     = swotpath          # carpeta con los .zip de SWOT\n",
    "output_dir  = swot_data         # carpeta de salida\n",
    "short_product = \"Node\"          # etiqueta para el archivo de salida\n",
    "ifplot      = True              # True = graficar, False = solo exportar\n",
    "valid_node_q = [0, 1]           # calidades válidas\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Lista de node_id a procesar (tal cual vienen en tu CSV/DataFrame) ---\n",
    "nodes_order = list(nodes_df['node ID'])\n",
    "nodes_set   = set(nodes_order)\n",
    "\n",
    "# --- Acumulador por node_id: NodeID -> [(Date, WSE, WSE_U, WIDTH), ...] ---\n",
    "accum = defaultdict(list)\n",
    "\n",
    "# --- Recorre cada ZIP una sola vez ---\n",
    "for filename in os.listdir(zip_dir):\n",
    "    if not (filename.lower().endswith(\".zip\") and \"node\" in filename.lower()):\n",
    "        continue\n",
    "    zip_path = os.path.join(zip_dir, filename)\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z, TemporaryDirectory(prefix=\"swot_nodes_\") as tmp_dir:\n",
    "            z.extractall(tmp_dir)\n",
    "\n",
    "            # Busca .shp (incluye subcarpetas)\n",
    "            shp_files = []\n",
    "            for root, _, files in os.walk(tmp_dir):\n",
    "                shp_files += [os.path.join(root, f) for f in files if f.lower().endswith(\".shp\")]\n",
    "            if not shp_files:\n",
    "                continue\n",
    "\n",
    "            for shp_path in shp_files:\n",
    "                try:\n",
    "                    gdf = gpd.read_file(shp_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] No se pudo leer {shp_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Mapeo case-insensitive\n",
    "                cols = {c.lower(): c for c in gdf.columns}\n",
    "                time_col    = cols.get('time_str') or cols.get('time')\n",
    "                wse_col     = cols.get('wse')\n",
    "                wse_u_col   = cols.get('wse_u')\n",
    "                node_id_col = cols.get('node_id')\n",
    "                node_q_col  = cols.get('node_q')\n",
    "                width_col   = cols.get('width')          # <-- NUEVO\n",
    "\n",
    "                if not (node_id_col and node_q_col and wse_col and time_col):\n",
    "                    continue\n",
    "\n",
    "                # Filtra temprano por tus node_id y calidad válida\n",
    "                mask = gdf[node_id_col].isin(nodes_set) & gdf[node_q_col].isin(valid_node_q)\n",
    "\n",
    "                # Columnas a conservar\n",
    "                keep_cols = [node_id_col, time_col, wse_col]\n",
    "                if wse_u_col:\n",
    "                    keep_cols.append(wse_u_col)\n",
    "                if width_col:                             # <-- NUEVO\n",
    "                    keep_cols.append(width_col)           # <-- NUEVO\n",
    "\n",
    "                sub = gdf.loc[mask, keep_cols].copy()\n",
    "                if sub.empty:\n",
    "                    continue\n",
    "\n",
    "                # Limpia y acumula\n",
    "                sub[\"Date\"] = pd.to_datetime(sub[time_col], errors=\"coerce\")\n",
    "                sub[wse_col] = pd.to_numeric(sub[wse_col], errors=\"coerce\")\n",
    "\n",
    "                if wse_u_col:\n",
    "                    sub[\"wse_u_tmp\"] = pd.to_numeric(sub[wse_u_col], errors=\"coerce\")\n",
    "                else:\n",
    "                    sub[\"wse_u_tmp\"] = pd.NA\n",
    "\n",
    "                if width_col:                             # <-- NUEVO\n",
    "                    sub[\"width_tmp\"] = pd.to_numeric(sub[width_col], errors=\"coerce\")\n",
    "                else:                                     # <-- NUEVO\n",
    "                    sub[\"width_tmp\"] = pd.NA              # <-- NUEVO\n",
    "\n",
    "                sub = sub.dropna(subset=[\"Date\", wse_col])\n",
    "\n",
    "                for nid, dt, wse, wseu, wid in zip(\n",
    "                        sub[node_id_col].tolist(),\n",
    "                        sub[\"Date\"].tolist(),\n",
    "                        sub[wse_col].tolist(),\n",
    "                        sub[\"wse_u_tmp\"].tolist(),\n",
    "                        sub[\"width_tmp\"].tolist()         # <-- NUEVO\n",
    "                    ):\n",
    "                    accum[nid].append((dt, wse, wseu, wid))  # <-- NUEVO\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"[WARN] ZIP corrupto o inválido: {zip_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Error procesando {zip_path}: {e}\")\n",
    "\n",
    "# --- Exporta y grafica SOLO los node_id de nodes_df (en su orden) ---\n",
    "for node_id in nodes_order:\n",
    "    pairs = accum.get(node_id, [])\n",
    "    if not pairs:\n",
    "        print(f\"No data found for node_id {node_id}.\")\n",
    "        continue\n",
    "\n",
    "    # ahora son tuplas (Date, WSE, WSE_u, Width)\n",
    "    pairs.sort(key=lambda x: x[0])  # ordenar por fecha\n",
    "    df = pd.DataFrame(\n",
    "        pairs,\n",
    "        columns=[\"Date\", \"WSE (m) EGM08\", \"WSE_u (m)\", \"Width (m)\"]  # <-- NUEVO\n",
    "    )\n",
    "\n",
    "    output_csv = os.path.join(output_dir, f\"{node_id}_{short_product}.csv\")\n",
    "    df.to_csv(output_csv, sep=';', decimal=',', encoding='utf-8', index=False)\n",
    "    print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "    if ifplot:\n",
    "        # Gráfico se mantiene SOLO con WSE\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df[\"Date\"], df[\"WSE (m) EGM08\"], marker='o', linestyle='-')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('WSE (m) EGM08')\n",
    "        plt.title(f'WSE over Time for Node ID: {node_id}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e378d-3db2-48d1-a8d4-8a5ec1a4aafb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())  # Muestra la carpeta actual de trabajo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2cca-771e-44dd-b1d0-ed35dc9ac523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
