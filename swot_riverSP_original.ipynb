{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWOT data acquisition\n",
    "\n",
    "#### Jonas Felipe Santos de Souza (jonas.ssouza@ufpe.br)\n",
    "\n",
    "#### Federal University of Pernambuco\n",
    "\n",
    "#### June 10, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANT:</b> check if the libraries below are installed.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, DrawControl, GeoJSON, Popup, Rectangle\n",
    "from shapely.geometry import Polygon\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import HTML\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import earthaccess\n",
    "import warnings\n",
    "import zipfile\n",
    "import time\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIRECTORY\n",
    "inpath = 'C:/Users/crist/Desktop/Doctorado/paper/SWOT/swot_rivers/'\n",
    "\n",
    "# Path to save the obtained products\n",
    "swotpath = f'{inpath}products/' # *.zip\n",
    "\n",
    "# SWOT ID of river sections (*.csv file)\n",
    "swot_id = f'{inpath}reachesidv17biobio.csv'\n",
    "\n",
    "# Path to save SWOT data after extraction\n",
    "swot_data = f'{inpath}River_SP_v17/reaches/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## River_SP SWOT database acquisition\n",
    "\n",
    "The database must be obtained from the *hydroweb.next* platform (https://hydroweb.next.theia-land.fr/).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>SKIP THIS STEP IF YOU ALREADY HAVE THE DATABASE YOU ARE INTERESTED IN.</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_message = \"\"\"\n",
    "Download products from your hydroweb.next projects (https://hydroweb.next.theia-land.fr) using the py-hydroweb lib (https://pypi.org/project/py-hydroweb/)\n",
    "This script is an example tuned for your last hydroweb.next project but feel free to adapt it for future requests.\n",
    "Follow these steps:\n",
    "1. If not already done, install py-hydroweb latest version using `pip install -U py-hydroweb` (WARNING: python >= 3.8 is required)\n",
    "2a. Generate an API-Key from hydroweb.next portal in your user settings\n",
    "2b. Carefully store your API-Key (2 options):\n",
    "- either in an environment variable `export HYDROWEB_API_KEY=\"<your_key_here>\"`\n",
    "- or in below script by replacing <your_key_here>\n",
    "3. You can change download directory by adding an `output_folder` parameter when calling `submit_and_download_zip` (see below). By default, current path is used.\n",
    "4. You are all set, run this script `python download_script.py`\n",
    "\n",
    "For more documentation about how to use the py-hydroweb lib, please refer to https://pypi.org/project/py-hydroweb/.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from importlib.metadata import version\n",
    "\n",
    "try:\n",
    "    import py_hydroweb\n",
    "except ImportError:\n",
    "    print(help_message)\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check py-hydroweb version\n",
    "latest_version = \"1.0.2\"\n",
    "if version(\"py_hydroweb\") < latest_version:\n",
    "    logging.getLogger().warning(f\"\"\"\\033[33m\n",
    "/!\\ Consider upgrading py-hydroweb to {latest_version} using `pip install -U py-hydroweb`\n",
    "\\033[0m\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set log config\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANT:</b> Check in your hydroweb.next account if the <b>API key</b> has been generated and is active.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key\n",
    "api_hydroweb = \"ICx9VJ6BM672ed61TIqHXDzwwtZRecK15AnYi9VlNF0ubKn5n9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client\n",
    "#  - either using the API-Key environment variable (HYDROWEB_API_KEY)\n",
    "#client: py_hydroweb.Client = py_hydroweb.Client(\"https://hydroweb.next.theia-land.fr/api\")\n",
    "#  - or explicitly giving API-Key (comment line above and uncomment line below)\n",
    "client: py_hydroweb.Client = py_hydroweb.Client(\"https://hydroweb.next.theia-land.fr/api\", \n",
    "                                                api_key=api_hydroweb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new download basket (input the name you want here)\n",
    "basket: py_hydroweb.DownloadBasket = py_hydroweb.DownloadBasket(\"my_download_basket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add collections in our basket\n",
    "# inserir no campo bbox as coordenadas da região de interesse\n",
    "# \"SWOT_PRIOR_RIVER_DATABASE\"\n",
    "# \"SWOT_PRIOR_LAKE_DATABASE\"\n",
    "basket.add_collection(\"SWOT_PRIOR_RIVER_DATABASE\", \n",
    "        #bbox=[-41.40, -9.60, -34.74, -7.10])\n",
    "        bbox=[-74.091797, -38.894373, -70.378418, -36.300877])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do download (input the archive name you want here, and optionally an output folder)\n",
    "now = datetime.today().strftime(\"%Y%m%dT%H%M%S\")\n",
    "downloaded_zip_path: str = client.submit_and_download_zip(\n",
    "    basket,\n",
    "    zip_filename=f\"{inpath}my_hydroweb_data_{now}.zip\",\n",
    "    #, output_folder = \"<change_me>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SWOT product search data configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapefile with reaches river\n",
    "# This file should be obtained from the SWOT database at hydroweb.next\n",
    "shp = f'{inpath}SWOT_PRIOR_RIVER_DATABASE/sa_sword_reaches_hb66_v17.shp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados para busca dos produtos SWOT\n",
    "# 'SWOT_L2_HR_LakeSP_Prior_2.0', 'SWOT_L2_HR_RiverSP_Reach_2.0', 'SWOT_L2_HR_Raster_100m_2.0'\n",
    "swot_product = 'SWOT_L2_HR_RiverSP_Reach_2.0'\n",
    "short_product = 'SWOT_RiverSP' # 'SWOT_LakeSP', 'SWOT_Raster', 'SWOT_RiverSP'\n",
    "date_start = '2024-01-01'\n",
    "date_end = '2024-12-31'\n",
    "granule_product = '*'\n",
    "\n",
    "# Plot graphs\n",
    "ifplot = True # True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Map for selecting the area of ​​interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create a map centered on the Amazon region\n",
    "m = Map(center=(-7.900, -35.250), zoom=8, layout={'height': '600px', 'width': '1000px'}, scroll_wheel_zoom=True)\n",
    "\n",
    "# Global variables to store the drawn polygon, the GeoJSON layer, and the rectangle layer\n",
    "polygon = None\n",
    "popup = None\n",
    "geojson_layer = None\n",
    "rectangle_layer = None\n",
    "\n",
    "# Step 2: Function to handle draw events\n",
    "def handle_draw(target, action, geo_json):\n",
    "    global polygon, geojson_layer, rectangle_layer\n",
    "    if action == 'created':\n",
    "        # Clear existing GeoJSON layer if it exists\n",
    "        if geojson_layer:\n",
    "            m.remove_layer(geojson_layer)\n",
    "\n",
    "        # Clear the existing rectangle layer if it exists\n",
    "        if rectangle_layer:\n",
    "            m.remove_layer(rectangle_layer)\n",
    "\n",
    "        # Clear the existing polygon layer if it exists\n",
    "        if polygon:\n",
    "            polygon_layer = GeoJSON(data={'type': 'Feature', 'geometry': polygon.__geo_interface__})\n",
    "            m.remove_layer(polygon_layer)\n",
    "\n",
    "        # Capture the polygon geometry\n",
    "        geometry = geo_json['geometry']\n",
    "        polygon = Polygon(geometry['coordinates'][0])\n",
    "        print(\"Polygon drawn:\", polygon)\n",
    "\n",
    "        # Draw the rectangle on the map\n",
    "        bounds = geometry['coordinates'][0]\n",
    "        rectangle_layer = Rectangle(bounds=bounds, color='blue', fill_opacity=0.1)\n",
    "        m.add_layer(rectangle_layer)\n",
    "\n",
    "        # Draw the new polygon layer\n",
    "        polygon_layer = GeoJSON(data={'type': 'Feature', 'geometry': polygon.__geo_interface__})\n",
    "        m.add_layer(polygon_layer)\n",
    "\n",
    "        load_shapefile()  # Load the shapefile based on the drawn polygon\n",
    "    elif action == 'deleted':\n",
    "        polygon = None\n",
    "        if geojson_layer:\n",
    "            m.remove_layer(geojson_layer)\n",
    "            geojson_layer = None\n",
    "        if rectangle_layer:\n",
    "            m.remove_layer(rectangle_layer)\n",
    "            rectangle_layer = None  # Reset rectangle_layer to None\n",
    "\n",
    "# Step 3: Load and filter the shapefile based on the drawn polygon\n",
    "def load_shapefile():\n",
    "    global polygon, geojson_layer\n",
    "    if polygon:\n",
    "        try:\n",
    "            gdf = gpd.read_file(shp)\n",
    "            print(f\"Shapefile carregado com sucesso: {len(gdf)} registros encontrados.\")\n",
    "            \n",
    "            gdf = gdf.to_crs(epsg=4326)\n",
    "            filtered_gdf = gdf[gdf.intersects(polygon)]\n",
    "            print(f\"{len(filtered_gdf)} registros encontrados dentro da área desenhada.\")\n",
    "            \n",
    "            geojson_data = filtered_gdf.__geo_interface__\n",
    "            \n",
    "            geojson_layer = GeoJSON(data=geojson_data, style={'color': 'green', 'opacity': 0.8, 'weight': 2})\n",
    "\n",
    "            # Add hover event to display 'lake_if' in a pop-up\n",
    "            def on_hover(event, feature, **kwargs):\n",
    "                global popup\n",
    "                lake_id = feature['properties'].get('reach_id', 'N/A')\n",
    "\n",
    "                geom = feature['geometry']\n",
    "                if geom['type'] == 'Polygon' or geom['type'] == 'MultiPolygon':\n",
    "                    coords = Polygon(geom['coordinates'][0]).centroid.coords[0]\n",
    "                elif geom['type'] == 'LineString':\n",
    "                    coords = Polygon(geom['coordinates']).centroid.coords[0]\n",
    "                else:\n",
    "                    coords = geom['coordinates']\n",
    "\n",
    "                # Create content for the popup with a selectable 'lake_id'\n",
    "                html_content = HTML(f'<div style=\"font-size: 14px;\"><b>Reach ID:</b> <span style=\"user-select: text;\">{lake_id}</span></div>')\n",
    "\n",
    "                # Remove the previous popup if it exists\n",
    "                if popup:\n",
    "                    m.remove_layer(popup)\n",
    "\n",
    "                # Create a new popup with the lake_id and add it to the map\n",
    "                popup = Popup(\n",
    "                    location=(coords[1], coords[0]),  # Coordinates (lat, lon)\n",
    "                    child=html_content,\n",
    "                    close_button=False,\n",
    "                    auto_close=False,\n",
    "                    close_on_escape_key=False\n",
    "                )\n",
    "                m.add_layer(popup)\n",
    "\n",
    "            geojson_layer.on_hover(on_hover)\n",
    "\n",
    "            m.add_layer(geojson_layer)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar o shapefile: {e}\")\n",
    "\n",
    "# Step 4: Add drawing tools to the map, allowing only rectangles\n",
    "draw_control = DrawControl(\n",
    "    rectangle={'shapeOptions': {'color': '#0000FF'}},  \n",
    "    polyline={}, polygon={}, circle={}, marker={}, circlemarker={}\n",
    ")\n",
    "draw_control.on_draw(handle_draw)\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# Step 5: Display the map\n",
    "\n",
    "# Compatibility helper for environments without Jupyter widget support\n",
    "def show_map_with_fallback(map_widget, fallback_file='swot_map_fallback.html'):\n",
    "    \"\"\"\n",
    "    Try to display the interactive ipyleaflet map.\n",
    "    If the frontend widget model is unavailable, save an HTML fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        display(map_widget)\n",
    "    except Exception as e:\n",
    "        print(f'Interactive widget could not be displayed: {e}')\n",
    "\n",
    "    map_widget.save(fallback_file)\n",
    "    print(\n",
    "        \"If you see 'Error displaying widget: model not found', \"\n",
    "        f'open the fallback map file: {fallback_file}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive map + HTML fallback for environments without widget support\n",
    "show_map_with_fallback(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Login and search for SWOT products in the *EarthData* database\n",
    "\n",
    "You need to have an EarthData account (https://urs.earthdata.nasa.gov/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANT:</b> Please make sure your EarthData account login and password are correct. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verificar se o polígono foi desenhado\n",
    "if 'polygon' in globals():   \n",
    "    # Earthdata login\n",
    "    earthaccess.login()\n",
    "    \n",
    "    # Buscar dados dentro dos limites do polígono\n",
    "    results = earthaccess.search_data(short_name = swot_product,\n",
    "                                      temporal = (date_start, date_end),\n",
    "                                      #granule_name=granule_product,\n",
    "                                      bounding_box=(polygon.bounds))\n",
    "    \n",
    "    # Exibir os granules encontrados\n",
    "    items = [item['meta']['native-id'] for item in results]\n",
    "    #print(f\"Granules encontrados: {items}\")\n",
    "else:\n",
    "    print(\"Nenhum polígono foi desenhado.\")\n",
    "\n",
    "# Display the granules found\n",
    "print(len(items))\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SWOT data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to download a file and retry if necessary\n",
    "def download_file_with_retries(file_url, download_path, max_retries=3):\n",
    "    attempts = 0\n",
    "    success = False\n",
    "\n",
    "    while attempts < max_retries and not success:\n",
    "        try:\n",
    "            earthaccess.download(file_url, download_path)\n",
    "            file = max(Path(download_path).glob('SWOT*.zip'), key=os.path.getmtime)\n",
    "            if file.exists() and file.stat().st_size > 0:\n",
    "                success = True\n",
    "                print(f\"Successfully downloaded: {file}\")\n",
    "            else:\n",
    "                raise Exception(\"File downloaded but appears to be incomplete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempts + 1} failed: {e}\")\n",
    "            attempts += 1\n",
    "            time.sleep(5)  # Wait for 5 seconds before retrying\n",
    "            # Clean up the incomplete file\n",
    "    if not success:\n",
    "        print(f\"Failed to download file after {max_retries} attempts.\")\n",
    "\n",
    "# Set the download directory and file URL (list of URLs in this example)\n",
    "file_urls = results[:500]  # Replace with your actual list of file URLs\n",
    "\n",
    "# Download files with retries\n",
    "for file_url in file_urls:\n",
    "    download_file_with_retries(file_url, swotpath)\n",
    "\n",
    "# Check the most recent file in the download directory\n",
    "files = glob.glob(swotpath + 'SWOT*.zip')\n",
    "try:\n",
    "    file = max(files, key=os.path.getmtime)\n",
    "    print(f\"\\nThe most recent file is: {file}\")\n",
    "    #assert file.exists()\n",
    "except ValueError:\n",
    "    print(\"\\nNo files were downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Capturing data of interest from downloaded SWOT products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>IMPORTANT:</b> It will help if you have a *.csv file with data on the sections of the rivers of interest (name, reaches SWOT ID etc.).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *.csv file with SWOT IDs of rivers\n",
    "reachid = pd.read_csv(swot_id, sep=';', decimal=',')\n",
    "reachid['reach ID'] = reachid['reach ID'].astype(str)\n",
    "reachid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the directory containing the zipped shapefiles\n",
    "zip_dir = swotpath\n",
    "output_dir = swot_data\n",
    "\n",
    "for i in range(len(reachid.index)):\n",
    "    # Define the target reach_id and other conditions\n",
    "    target_reach_id = reachid['reach ID'][i]\n",
    "    #target_reach_id = '62281100021'\n",
    "    valid_reach_q = [0, 1]\n",
    "\n",
    "    # Lists to store the data\n",
    "    records = []\n",
    "\n",
    "    # Loop over all ZIP files in the directory\n",
    "    for filename in os.listdir(zip_dir):\n",
    "        if filename.endswith(\".zip\") and \"Reach\" in filename:\n",
    "            zip_path = os.path.join(zip_dir, filename)\n",
    "            \n",
    "            with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "                # Extract all the shapefile components (shp, shx, dbf, etc.)\n",
    "                z.extractall(\"temp_shapefile\")  # Temporary folder to hold unzipped files\n",
    "                shapefile_path = [f for f in os.listdir(\"temp_shapefile\") if f.endswith(\".shp\")][0]\n",
    "                shapefile_full_path = os.path.join(\"temp_shapefile\", shapefile_path)\n",
    "                \n",
    "                # Load the shapefile into a GeoDataFrame\n",
    "                gdf = gpd.read_file(shapefile_full_path)\n",
    "\n",
    "                # Mapeo case-insensitive de columnas\n",
    "                cols = {c.lower(): c for c in gdf.columns}\n",
    "                reach_id_col  = cols.get('reach_id')\n",
    "                reach_q_col   = cols.get('reach_q')\n",
    "                time_col      = cols.get('time_str') or cols.get('time')\n",
    "                wse_col       = cols.get('wse')\n",
    "                wse_u_col     = cols.get('wse_u')\n",
    "                slope_col     = cols.get('slope')\n",
    "                slope_u_col   = cols.get('slope_u')\n",
    "                width_col     = cols.get('width')\n",
    "                width_u_col   = cols.get('width_u')\n",
    "                area_total_col= cols.get('area_total')\n",
    "                area_detct_col= cols.get('area_detct')\n",
    "                area_tot_u_col= cols.get('area_tot_u')\n",
    "                area_det_u_col= cols.get('area_det_u')\n",
    "\n",
    "                if not (reach_id_col and reach_q_col and time_col and wse_col):\n",
    "                    # Clean up the temporary folder\n",
    "                    for f in os.listdir(\"temp_shapefile\"):\n",
    "                        os.remove(os.path.join(\"temp_shapefile\", f))\n",
    "                    continue\n",
    "                \n",
    "                # Filter the data based on reach_id and reach_q\n",
    "                filtered_gdf = gdf[(gdf[reach_id_col] == target_reach_id) & (gdf[reach_q_col].isin(valid_reach_q))].copy()\n",
    "                \n",
    "                # Extract variables\n",
    "                if not filtered_gdf.empty:\n",
    "                    filtered_gdf['Date'] = pd.to_datetime(filtered_gdf[time_col], errors='coerce')\n",
    "                    filtered_gdf['wse'] = pd.to_numeric(filtered_gdf[wse_col], errors='coerce')\n",
    "\n",
    "                    extra_map = {\n",
    "                        'wse_u': wse_u_col,\n",
    "                        'slope': slope_col,\n",
    "                        'slope_u': slope_u_col,\n",
    "                        'width': width_col,\n",
    "                        'width_u': width_u_col,\n",
    "                        'area_total': area_total_col,\n",
    "                        'area_detct': area_detct_col,\n",
    "                        'area_tot_u': area_tot_u_col,\n",
    "                        'area_det_u': area_det_u_col,\n",
    "                    }\n",
    "\n",
    "                    for out_col, in_col in extra_map.items():\n",
    "                        if in_col:\n",
    "                            filtered_gdf[out_col] = pd.to_numeric(filtered_gdf[in_col], errors='coerce')\n",
    "                        else:\n",
    "                            filtered_gdf[out_col] = pd.NA\n",
    "\n",
    "                    filtered_gdf = filtered_gdf.dropna(subset=['Date', 'wse'])\n",
    "\n",
    "                    for row in filtered_gdf[['Date', 'wse', 'wse_u', 'slope', 'slope_u', 'width', 'width_u', 'area_total', 'area_detct', 'area_tot_u', 'area_det_u']].itertuples(index=False, name=None):\n",
    "                        records.append(row)\n",
    "            \n",
    "                # Clean up the temporary folder\n",
    "                for f in os.listdir(\"temp_shapefile\"):\n",
    "                    os.remove(os.path.join(\"temp_shapefile\", f))\n",
    "    \n",
    "    # Check if data was found\n",
    "    if records:\n",
    "        # Sort the values by time\n",
    "        records.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Step to save data to a CSV file\n",
    "        df = pd.DataFrame(\n",
    "            records,\n",
    "            columns=['Date', 'wse', 'wse_u', 'slope', 'slope_u', 'width', 'width_u', 'area_total', 'area_detct', 'area_tot_u', 'area_det_u']\n",
    "        )\n",
    "    \n",
    "        # Save DataFrame to CSV, using the reach_id as the filename\n",
    "        output_csv = os.path.join(output_dir, f'{target_reach_id}_{short_product}.csv')\n",
    "        df.to_csv(output_csv, sep=',', decimal='.', encoding='utf-8', index=False)\n",
    "        print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "        if ifplot == True:\n",
    "            # Plotting\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(df['Date'], df['wse'], marker='o', linestyle='-')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('WSE (m) EGM08')\n",
    "            plt.title(f'WSE over Time for Reach ID: {target_reach_id}')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(f\"No data found for reach_id {target_reach_id}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}